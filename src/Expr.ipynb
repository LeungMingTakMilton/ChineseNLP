{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "from langconv import *\n",
    "jieba.load_userdict(\"dict.txt.big.txt\")\n",
    "jieba.load_userdict(\"dict.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "    \n",
    "# create sample documents\n",
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\" \n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.086*health + 0.086*brocolli + 0.086*good'), (1, u'0.068*mother + 0.068*brother + 0.068*drive')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=3, num_words=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(text.count('health'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2), (1, 2), (2, 1), (3, 1), (4, 1), (5, 2)]\n",
      "[(3, 1), (4, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)]\n",
      "[(8, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1)]\n",
      "[(3, 1), (4, 1), (8, 1), (18, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1)]\n",
      "[(0, 1), (1, 1), (19, 2), (30, 1), (31, 1)]\n"
     ]
    }
   ],
   "source": [
    "for vector in corpus:\n",
    "    print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "content=[]\n",
    "with open(\"essay\") as f:\n",
    "    for line in f:\n",
    "        line = Converter('zh-hant').convert(line.decode('utf-8'))\n",
    "        content.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "剛剛擊敗世界圍棋冠軍的 AlphaGo，是怎樣「思考」的？\"\n",
      "\n",
      "剛剛 / 擊敗 / 世界圍棋 / 冠軍 / 的 /   / AlphaGo / ， / 是 / 怎樣 / 「 / 思考 / 」 / 的 / ？ / \" / \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demo of word segmentation algorithm\n",
    "print content[0]\n",
    "print \" / \".join(jieba.cut(content[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if a word is all CJK characters\n",
    "def isCJK(w):\n",
    "    if not w.isalpha():\n",
    "        return False\n",
    "    # for c in w:\n",
    "    #     n = unicodedata.name(unicode(c))\n",
    "    #     if not n.startswith(\"CJK\"):\n",
    "    #         return False\n",
    "    return True\n",
    "\n",
    "# loop through document list\n",
    "import unicodedata\n",
    "texts=[]\n",
    "for i in content:\n",
    "    # clean and tokenize document string\n",
    "    tokens=[]\n",
    "    for w in jieba.cut(i):\n",
    "        if len(w)>1:\n",
    "            if isCJK(w):\n",
    "                tokens.append(w.strip())\n",
    "    texts.append(tokens)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency Count: \n",
      "(AlphaGo, 1)  (一個, 1)  (大腦, 1)  (落子, 1)  (選擇器, 1)  (第二個, 1)  (問題, 1)  (回答, 1)  (相對, 1) \n"
     ]
    }
   ],
   "source": [
    "## Frequency Count\n",
    "print \"Frequency Count: \"\n",
    "for id, cnt in corpus[40]:\n",
    "    print \"(%s, %d) \" % (dictionary[id], cnt),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term weighting by TF-IDF: \n",
      "(冠軍, 0.23)  (AlphaGo, 0.06)  (歐洲, 0.27)  (首次, 0.27)  (研發, 0.27)  (宣佈, 0.27)  (選手, 0.20)  (AI, 0.12)  (職業, 0.20)  (谷歌, 0.27)  (Hui, 0.27)  (圍棋, 0.28)  (戰勝, 0.27)  (他們, 0.15)  (人類, 0.13)  (二段, 0.27)  (Fan, 0.27)  (DeepMind, 0.27)  (神經網絡, 0.13) \n"
     ]
    }
   ],
   "source": [
    "print \"Term weighting by TF-IDF: \"\n",
    "for id, score in tfidf[corpus[1]]:\n",
    "    print \"(%s, %.2f) \" % (dictionary[id], score),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(tfidf[corpus], num_topics=2, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.005*大腦 + 0.005*AlphaGo + 0.005*判斷 + 0.004*落子 + 0.004*閱讀 + 0.004*局面 + 0.004*可能 + 0.004*如果 + 0.004*這個 + 0.004*下一步\n",
      "1 0.006*訓練 + 0.006*通過 + 0.005*一個 + 0.005*落子 + 0.005*圍棋 + 0.005*他們 + 0.004*選擇器 + 0.004*使用 + 0.004*網絡 + 0.004*AlphaGo\n"
     ]
    }
   ],
   "source": [
    "for index,topic in ldamodel.print_topics(num_topics=2, num_words=10):\n",
    "    print index,topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsi = models.LsiModel(tfidf[corpus], id2word=dictionary, num_topics=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "落子 -0.223021077592\n",
      "選擇器 -0.187362307902\n",
      "局面 -0.185162691145\n",
      "大腦 -0.178620820471\n",
      "通過 -0.168648805455\n",
      "訓練 -0.146857883844\n",
      "AlphaGo -0.146011080108\n",
      "閱讀 -0.144261517574\n",
      "評估器 -0.141535235406\n",
      "他們 -0.139948636181\n"
     ]
    }
   ],
   "source": [
    "for i,j in lsi.show_topic(0)[:10]: print i,j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.interfaces.TransformedCorpus object at 0x116759210>\n"
     ]
    }
   ],
   "source": [
    "print tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "冠軍 1\n",
      "AlphaGo 1\n",
      "歐洲 1\n",
      "首次 1\n",
      "研發 1\n",
      "宣佈 1\n",
      "選手 1\n",
      "AI 1\n",
      "職業 1\n",
      "谷歌 1\n",
      "Hui 1\n",
      "圍棋 2\n",
      "戰勝 1\n",
      "他們 1\n",
      "人類 1\n",
      "二段 1\n",
      "Fan 1\n",
      "DeepMind 1\n",
      "神經網絡 1\n"
     ]
    }
   ],
   "source": [
    "for i,j in corpus[1]: print dictionary[i],j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "document = [\"圍棋\", \"選擇器\"]\n",
    "dictionary.add_documents([document])\n",
    "# dictionary.doc2bow(document) \n",
    "corpus.append(dictionary.doc2bow(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " a = dict(one=1, two=2, three=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.get('one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "china = \"DeepMind\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(china)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jieba."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
