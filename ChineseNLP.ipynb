{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# FYP15002 Chinese character and word analysis in daily essays - Natural Langugage Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### About Author\n",
    "- Milton Leung\n",
    "- 3035053381\n",
    "- mingtak@connect.hku.hk\n",
    "\n",
    "### About presentation slides\n",
    "- HTML code produced by ipython notebook\n",
    "- Powered by reveal.js\n",
    "\n",
    "### About presentation\n",
    "- Background: Applications, NLP Pipeline\n",
    "- Chinese NLP: Segmentation, algorithms and demonstration\n",
    "- Case Study: Training models on Chinese Wikipedia\n",
    "\n",
    "\n",
    "### About working environment\n",
    "- Unix-like environment\n",
    "- Minimum 2GB RAMs\n",
    "- Python 2.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Aouut packages and tools\n",
    "- External packages: gensim, jieba; pip install gensim, jieba\n",
    "- Working tools: IPython Notebook and Jupyter: An interactive computational environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Image source: <a href=http://ipython.org/_static/sloangrant/9_home_fperez_prof_grants_1207-sloan-ipython_proposal_fig_ipython-notebook-specgram.png>http://ipython.org/_static/sloangrant/9_home_fperez_prof_grants_1207-sloan-ipython_proposal_fig_ipython-notebook-specgram.png</a>\n",
    "<img src=http://ipython.org/_static/sloangrant/9_home_fperez_prof_grants_1207-sloan-ipython_proposal_fig_ipython-notebook-specgram.png width=50% height=50%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import codecs\n",
    "import sys\n",
    "import unicodedata\n",
    "import pickle\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "from langconv import *\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Project Pipeline\n",
    "<img src=img/ProjectFlow.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why NLP?\n",
    "- Support searching and retrieval of documents\n",
    "- Extract key sentences from documents\n",
    "- Identify similar documents\n",
    "- Identify major topics of in a group of documents\n",
    "- Detect sentiment in user comments\n",
    "- ...\n",
    "- NLP is fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chinese Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Chinese NLP \n",
    "- Sentence = \"你好世界\"\n",
    "- How to segment a sentence into words?\n",
    "- (你好世界), (你好 / 世界), (你 / 好世界),  (你好世 / 界), (你 / 好 / 世界)  ......\n",
    "<img src=http://thumbs.dreamstime.com/x/你好世界-11059717.jpg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Word Segmentation Algorithm\n",
    "- Based on a prefix dictionary structure, we achieve efficient word graph scanning. \n",
    "<img src=img/Picture1.png  width=\"50%\" height=\"50%\"> \n",
    "- Build a directed acyclic graph (DAG) for all possible word combinations.\n",
    "- Use dynamic programming to find the most probable combination based on the word frequency.\n",
    "<img src=img/helloWorld.png  width=\"50%\" height=\"50%\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- For unknown words, a HMM-based model is used with the Viterbi algorithm.\n",
    "<img src=https://upload.wikimedia.org/wikipedia/commons/7/73/Viterbi_animated_demo.gif>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Demo: Word Segmentation by Jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Jieba (結巴) - Chinese Word Segmentation Module\n",
    "- Support Simpified and Traditional Chinese Segmentations\n",
    "- Allow to use user defined Dictionaries\n",
    "- MIT License\n",
    "- GitHub: <a href=https://github.com/fxsjy/jieba>https://github.com/fxsjy/jieba</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好 / 世界\n"
     ]
    }
   ],
   "source": [
    "helloWorld = \"你好世界\"\n",
    "print \" / \".join(jieba.cut(helloWorld))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Part-of-Speech Tagging \n",
    "<a href=https://gist.github.com/luw2007/6016931#ictclas-汉语词性标注集>https://gist.github.com/luw2007/6016931#ictclas-汉语词性标注集</a>\n",
    "- l\t习用语\t习用语尚未成为成语，有点“临时性”，取“临”的声母。\n",
    "- n\t名词\t取英语名词noun的第1个字母。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好 l\n",
      "世界 n\n"
     ]
    }
   ],
   "source": [
    "words = pseg.cut(helloWorld)\n",
    "for word, flag in words:\n",
    "    print word, flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一  般 直 些 時 定 月 點 一 旦 切 樣 再 起 致 度 路 向 方面 處 帶 面 併 塊 律 連串 邊 舉 生 口氣 味 貫 道 同 陣子 則 連 旁 流 手 體 共 般而言 來 如 早 經 下子 心 角 大早 概 窩蜂 席之地 干 時之間 觸即發 字 發不可收拾 枝獨秀 會兒 會 落千丈 面倒 心一意 帆風順 模一樣 髮千鈞 舉兩得 蹶不振 籌莫展 覽無遺\r\n",
      "乙  烯 炔 二醇 卯 酸\r\n",
      "丁  二烯 烷 香 憂\r\n",
      "七  星 彩 色 嘴八舌 折八扣 情 零八落 情六慾 竅 言絕句 言律詩 彩繽紛 十二烈士 手八腳 律 拼八湊 里香\r\n",
      "乃  至 父 弟\r\n",
      "......\r\n",
      "攢  錢\r\n",
      "鱈  魚\r\n",
      "鰲  頭\r\n",
      "鷸  蚌相爭 蚌相爭漁翁得利\r\n",
      "顴  骨\r\n"
     ]
    }
   ],
   "source": [
    "# Chinese dictionary from Dr. Vincent Lau\n",
    "!echo \"`head -5 dict`\\n......\\n`tail -5 dict`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一般\r\n",
      "一直\r\n",
      "一些\r\n",
      "一時\r\n",
      "一定\r\n",
      "......\r\n",
      "鱈魚\r\n",
      "鰲頭\r\n",
      "鷸蚌相爭\r\n",
      "鷸蚌相爭漁翁得利\r\n",
      "顴骨\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"`head -5 dict.txt`\\n......\\n`tail -5 dict.txt`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "數學 / （ / Mathematics / ） / 是 / 利用 / 符號 / 語言 / 研究 / 數量 / 、 / 結構 / 、 / 變化 / 以及 / 空間 / 等 / 概念 / 的 / 一門 / 學科 / ， / \n",
      " / 從 / 某種 / 角度看 / 屬 / 於 / 形式 / 科學 / 的 / 一種 / 。 / 數學 / 透過 / 抽象化 / 和 / 邏輯 / 推理 / 的 / 使用 / ， / 由計數 / 、 / 計算 / 、 / 量度 / 和 / 對 / 物體 / 形狀及 / 運動 / 的 / 觀察而產生 / 。 / \n",
      " / 數學家們 / 拓展 / 這些 / 概念 / ， / 為 / 了 / 公式化 / 新 / 的 / 猜想 / 以及 / 從 / 選定 / 的 / 公理 / 及定 / 義中 / 建立 / 起 / 嚴謹 / 推導出 / 的 / 定理 / 。\n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"數學（Mathematics）是利用符號語言研究數量、結構、變化以及空間等概念的一門學科，\n",
    "從某種角度看屬於形式科學的一種。數學透過抽象化和邏輯推理的使用，由計數、計算、量度和對物體形狀及運動的觀察而產生。\n",
    "數學家們拓展這些概念，為了公式化新的猜想以及從選定的公理及定義中建立起嚴謹推導出的定理。\"\"\"\n",
    "segment1= \" / \".join(jieba.cut(sentence))\n",
    "print segment1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's include our dictionaries!\n",
    "jieba.load_userdict(\"dict.txt.big.txt\")\n",
    "jieba.load_userdict(\"dict.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After:\n",
      "數學 / （ / Mathematics / ） / 是 / 利用 / 符號語言 / 研究 / 數量 / 、 / 結構 / 、 / 變化 / 以及 / 空間 / 等 / 概念 / 的 / 一門 / 學科 / ， / \n",
      " / 從 / 某種 / 角度看 / 屬於 / 形式 / 科學 / 的 / 一種 / 。 / 數學 / 透過 / 抽象化 / 和 / 邏輯推理 / 的 / 使用 / ， / 由 / 計數 / 、 / 計算 / 、 / 量度 / 和 / 對 / 物體 / 形狀 / 及 / 運動 / 的 / 觀察 / 而 / 產生 / 。 / \n",
      " / 數學家 / 們 / 拓展 / 這些 / 概念 / ， / 為 / 了 / 公式化 / 新 / 的 / 猜想 / 以及 / 從 / 選定 / 的 / 公理 / 及 / 定義 / 中 / 建立起 / 嚴謹 / 推導 / 出 / 的 / 定理 / 。\n",
      "數學,一門,數學家,變化,計算\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse\n",
    "segment2=\" / \".join(jieba.cut(sentence))\n",
    "print \"After:\\n\"+ segment2\n",
    "tags = jieba.analyse.extract_tags(segment2, 5)\n",
    "print \",\".join(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Case Study:  Training Models on Chinese Wikipedia by Gensim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Case Study:  Training Models on Chinese Wikipedia by Gensim\n",
    "- Data Source: <a href=http://dumps.wikimedia.org/zhwiki/latest/>http://dumps.wikimedia.org/zhwiki/latest/</a>\n",
    "- We use zhwiki-latest-abstract.xml as training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### NLP Pipeline\n",
    "- Data Preparation: Word extraction\n",
    "- Preprocessing: Word normalization, segmentation etc. \n",
    "- Modelling: Training, validaton, analysis etc.\n",
    "<img src=\"http://www.cis.uni-muenchen.de/~davidk/ap/res/img/nlp-pipeline.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Cleaning and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "997M\tzhwiki-latest-abstract.xml\n",
      "<feed>\n",
      "<doc>\n",
      "<title>Wikipedia：数学</title>\n",
      "<url>https://zh.wikipedia.org/wiki/%E6%95%B0%E5%AD%A6</url>\n",
      "<abstract>数学（Mathematics）是利用符号语言研究數量、结构、变化以及空间等概念的一門学科，从某种角度看屬於形式科學的一種。數學透過抽象化和邏輯推理的使用，由計數、計算、量度和對物體形狀及運動的觀察而產生。數學家們拓展這些概念，為了公式化新的猜想以及從選定的公理及定義中建立起嚴謹推導出的定理。Jourdain</abstract>\n",
      " 14134285 zhwiki-latest-abstract.xml\n"
     ]
    }
   ],
   "source": [
    "# Data Source: wiki-dumped abstracts up to 4 January 2016\n",
    "# Uncompressed xml file, around 1GB\n",
    "!du -h zhwiki-latest-abstract.xml; \n",
    "!head -5 zhwiki-latest-abstract.xml; \n",
    "!wc -l zhwiki-latest-abstract.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Data Cleaning and Normalization\n",
    "- Extracted target tags from xml\n",
    "- Filter out short abstracts\n",
    "- Convert simplified Chinese to Traditional Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "數學（Mathematics）是利用符號語言研究數量、結構、變化以及空間等概念的一門學科，從某種角度看屬於形式科學的一種。數學透過抽象化和邏輯推理的使用，由計數、計算、量度和對物體形狀及運動的觀察而產生。數學家們拓展這些概念，為了公式化新的猜想以及從選定的公理及定義中建立起嚴謹推導出的定理。Jourdain\r\n",
      "哲學（，），源於日語對philosophy的翻譯，“哲學”一詞，不但可以是特指學術上針對思考方法的學問，也是指對普遍而基本的主題的研究，這些主題多與實在、存在、知識、價值、理性、心靈、語言、思想等有關。哲學與其他學科的不同之處在於其批判的方式、通常是系統化的方法，並且以理性論證為基礎。\r\n",
      "文學是指以語言文字為工具形象化地反映藝術，包括戲劇、詩歌、小說、散文等，是文化的重要表現形式，以不同的形式（稱作體裁）表現內心情感和再現一定時期和一定地域的社會生活、文化。\r\n",
      "歷史，或簡稱史，指人類社會過去的事件和行動，以及對這些事件行為有系統的記錄、詮釋和研究。歷史可提供今人理解過去，作為未來行事的參考依據，與倫理、哲學和藝術同屬人類精神文明的重要成果。歷史的第二個含義，即對過去事件的記錄和研究，又稱為“歷史學”，或簡稱“史學”。隸屬於歷史學或與其密切相關的學科有年代學、編纂學、家譜學、古文字學、計量歷史學、考古學、社會學和新聞學等，參見歷史學。習罡華，《歷史是什麼？——一項純形而上學的思考》詞源 ==\r\n",
      "族群（），是指一群人，他們認為彼此共享了相同的祖先、血緣、外貌、歷史、文化、習俗、語言、地域、宗教、生活習慣與國家體驗等，因此形成一個共同的群體。為區分我族及「他者」的分類方式之一。族群含義在20世紀後有轉變，從原來以少數民族或少數族裔的意思，到後來以文化特徵區分，而最新的看法則認為族群是社會過程後的產生的結果。因此，族族可能因歷史及時空環境，基於歷史、文化、語言、地域、宗教、血緣祖先認同、行為、生物/外貌特征而形成「一群」與其它有所區別的群體。p.456 &quot;The ideas of ethnicity and ethnic group have a long history, often related to &quot;otherness&quot;. In the 20th century and beyond, the idea of what constitutes an ethnic group has changed, once associated with minority status and later with cultural characteristics, ethnicity is most recently viewed as the outcome of a social process&quot; 這些區別我者�\r\n"
     ]
    }
   ],
   "source": [
    "# After extracting xml tags\n",
    "# Over 227,000 abstracts extracted\n",
    "!head -5 texts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  227364 6932865 48963468 docs.txt\n",
      "數學 利用 符號語言 研究 數量 結構 變化 以及 空間 概念 一門 學科 某種 角度看 屬於 形式 科學 一種 數學 透過 抽象化 邏輯推理 使用 計數 計算 量度 物體 形狀 運動 觀察 產生 數學家 拓展 這些 概念 公式化 猜想 以及 選定 公理 定義 建立起 嚴謹 推導 定理\n",
      "哲學 源於 日語 翻譯 哲學 一詞 不但 可以 特指 學術 針對 思考 方法 學問 普遍 基本 主題 研究 這些 主題 實在 存在 知識 價值 理性 心靈 語言 思想 有關 哲學 與其 學科 不同之處 在於 批判 方式 通常 系統化 方法 並且 理性 論證 基礎\n",
      "文學 指以 語言文字 工具 形象化 反映 藝術 包括 戲劇 詩歌 小說 散文 文化 重要 表現形式 不同 形式 稱作 體裁 表現 內心 情感 再現 一定 時期 一定 地域 社會 生活 文化\n",
      "歷史 簡稱 人類 社會 過去 事件 行動 以及 這些 事件 行為 系統 記錄 詮釋 研究 歷史 提供 今人 理解 過去 作為 未來 行事 參考 依據 倫理 哲學 藝術 同屬 人類 精神文明 重要 成果 歷史 第二個 含義 過去 事件 記錄 研究 又稱 歷史學 簡稱 史學 隸屬於 歷史學 與其 密切相關 學科 年代學 編纂 家譜 古文字學 計量 歷史學 考古學 社會學 新聞學 參見 歷史學 習罡華 歷史 什麼 一項 形而上學 思考 詞源\n",
      "族群 一群 他們 認為 彼此 共享 相同 祖先 血緣 外貌 歷史 文化 習俗 語言 地域 宗教 生活習慣 國家 體驗 因此 形成 一個 共同 群體 區分 我族 他者 分類 方式 之一 族群 含義 世紀 轉變 原來 少數民族 少數 族裔 意思 後來 文化 特徵 區分 最新 法則 認為 族群 社會 過程 產生 結果 因此 族族 可能 歷史 及時 環境 基於 歷史 文化 語言 地域 宗教 血緣 祖先 認同 行為 生物 外貌 特征 形成 一群 與其 有所區別 群體 這些 區別 我者\n",
      "戲劇 演員 某個 故事 情境 對話 歌唱 動作 方式 表演 出來 藝術 戲劇 四個 元素 包括 演員 故事 情境 舞臺 表演 場地 觀眾\n",
      "經濟學 一門 產品 服務 生產 分配 以及 消費 進行 研究 社會科學 這一 名詞 源於 古希臘 起初 這一 領域 稱為 政治經濟學 世紀 經濟學家 採用 簡短 經濟學 一詞 代表 經濟 科學 這也 避免 誤解 政治學 數學 倫理學 領域\n",
      "政治學 一門 研究 政治 行為 政治體制 以及 政治 相關 領域 為主 社會科學 學科 西方 政治學 學術 領域 裡的 研究 稱為 政治 研究 政治 科學 只有 政治 兩字 政治學 意味著 學術 研究 領域 政治 研究 代表 廣泛 研究 領域\n",
      "社會學 起源於 世紀 末期 一門 研究 社會 學科 社會學 使用 各種 研究 方法 進行 實證 調查 批判 分析 發展 完善 一套 有關 人類 社會 結構 活動 知識 體系 並會 運用 這些 知識 尋求 改善 社會福利 目標\n",
      "信息學 臺灣 資訊 中國 信息學 又稱 信息 資訊 科學 資訊 信息 科學 舊稱 情報學 外來語 主要 指以 信息 研究 對象 利用計算機 及其 程序設計 技術 研究 工具 分析 問題 解決問題 學問 是以 擴展 人類 信息 功能 主要 目標 一門 綜合性 學科\n"
     ]
    }
   ],
   "source": [
    "# After segmentation\n",
    "# Over 6 million Chinese word segemnts \n",
    "# Around 49 million Chingese characters\n",
    "!wc docs.txt\n",
    "!head docs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dictionary and Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Build dictionary and corpora from documents\n",
    "- Corpora: Collection of documents\n",
    "- Document: A bag of word segments\n",
    "- Word segment: Also known as token, basic unit in documents, abstracts or sentences\n",
    "- Dictionary: Collection of all word segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(73710 unique tokens: [u'', u'\\u71c8\\u9b5a', u'\\u5f35\\u4e7e\\u6587', u'\\u6885\\u6d1b', u'\\u7a0b\\u9865']...)\n"
     ]
    }
   ],
   "source": [
    "# Build corpus from abstracts\n",
    "# the segmented abstracts are stored in docs.txt, one per line\n",
    "infile = codecs.open(\"docs.txt\", \"r\", \"utf-8\") \n",
    "documents = [ l.strip().split(\" \") for l in infile ] # documents is a list of list\n",
    "dictionary = corpora.Dictionary(documents) # generate the dictionary \n",
    "dictionary.filter_extremes(no_below=5) # ignore words appear less than 5 times\n",
    "corpus = [ dictionary.doc2bow(d) for d in documents ] \n",
    "# Around 70,000 unique tokens\n",
    "print dictionary \n",
    "# Export as wiki dictionary\n",
    "dictionary.save_as_text(\"wikiDict.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "數學 利用 符號語言 研究 數量 結構 變化 以及 空間 概念 一門 學科 某種 角度看 屬於 形式 科學 一種 數學 透過 抽象化 邏輯推理 使用 計數 計算 量度 物體 形狀 運動 觀察 產生 數學家 拓展 這些 概念 公式化 猜想 以及 選定 公理 定義 建立起 嚴謹 推導 定理\n"
     ]
    }
   ],
   "source": [
    "# Words token from \n",
    "print \" \".join(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Frequency Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Frequency Count\n",
    "- Find most frequenct words from abstract \"數學\"\n",
    "- Find most common words in Chinese Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(概念, 2)  (以及, 2)  (數學, 2)  (這些, 1)  (運動, 1)  (定義, 1)  (透過, 1)  (觀察, 1)  (使用, 1)  (抽象化, 1)  (公式化, 1)  (物體, 1)  (學科, 1)  (形式, 1)  (建立起, 1)  (一門, 1)  (利用, 1)  (屬於, 1)  (科學, 1)  (量度, 1)  (變化, 1)  (產生, 1)  (結構, 1)  (猜想, 1)  (空間, 1)  (公理, 1)  (角度看, 1)  (一種, 1)  (選定, 1)  (研究, 1)  (計數, 1)  (某種, 1)  (計算, 1)  (嚴謹, 1)  (定理, 1)  (拓展, 1)  (邏輯推理, 1)  (推導, 1)  (數量, 1)  (形狀, 1)  (數學家, 1) \n"
     ]
    }
   ],
   "source": [
    "# Frequency count from first abstract \"數學\"\n",
    "\n",
    "corpus[0].sort(key=lambda x: x[1], reverse=True)\n",
    "for id, cnt in corpus[0]:\n",
    "    print \"(%s, %d) \" % (dictionary[id], cnt),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28687\t位於\t54606\r\n",
      "50179\t一個\t44078\r\n",
      "13039\t中國\t37696\r\n",
      "55647\t分佈\t31973\r\n",
      "39366\t一種\t29991\r\n",
      "5334\t其中\t21486\r\n",
      "28668\t平方公里\t20865\r\n",
      "13277\t地區\t18431\r\n",
      "10507\t學名\t17967\r\n",
      "29576\t棲息\t16356\r\n"
     ]
    }
   ],
   "source": [
    "# Sort by frequency count\n",
    "!sort -k 3 -rn wikiDict.txt > sortedWikiDict.txt\n",
    "!head sortedWikiDict.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Analysis apart from frequency count?\n",
    "- ... Fullstop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Term weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Term weighting\n",
    "- Evaluate word importance\n",
    "- Frequent words != important words\n",
    "- High word count of \"一個\" != important\n",
    "- What is importance meant by computer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Condition of an important word \n",
    "- The word should appear often in a document\n",
    "- The word should *NOT* appear often in other documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### TF-IDF\n",
    "- Term Frequency - Inverse Document Frequency \n",
    "- Method to quantify the importance given a document\n",
    "- Product of two statistics (TF x IDF)\n",
    "- Central tool of search engines for scoring and ranking a document's relevance\n",
    "<img src=https://upload.wikimedia.org/math/e/5/a/e5a7b43197068eddf42859f3995ebf15.png>\n",
    "<img src=https://upload.wikimedia.org/math/0/2/5/0257ce95c505ab568d7898faa56a4f5c.png>\n",
    "<img src=https://upload.wikimedia.org/math/b/0/6/b06a060c28253c8dd2528811c447862e.png>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's build the TF-IDF model\n",
    "tfidf = models.TfidfModel(corpus) \n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency Count: \n",
      "(概念, 2)  (以及, 2)  (數學, 2)  (這些, 1)  (運動, 1)  (定義, 1)  (透過, 1)  (觀察, 1)  (使用, 1)  (抽象化, 1)  (公式化, 1)  (物體, 1)  (學科, 1)  (形式, 1)  (建立起, 1)  (一門, 1)  (利用, 1)  (屬於, 1)  (科學, 1)  (量度, 1)  (變化, 1)  (產生, 1)  (結構, 1)  (猜想, 1)  (空間, 1)  (公理, 1)  (角度看, 1)  (一種, 1)  (選定, 1)  (研究, 1)  (計數, 1)  (某種, 1)  (計算, 1)  (嚴謹, 1)  (定理, 1)  (拓展, 1)  (邏輯推理, 1)  (推導, 1)  (數量, 1)  (形狀, 1)  (數學家, 1) \n"
     ]
    }
   ],
   "source": [
    "## Frequency Count\n",
    "print \"Frequency Count: \"\n",
    "for id, cnt in corpus[0]:\n",
    "    print \"(%s, %d) \" % (dictionary[id], cnt),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term weighting by TF-IDF: \n",
      "(一門, 0.14)  (數學家, 0.13)  (量度, 0.18)  (變化, 0.12)  (計算, 0.12)  (利用, 0.11)  (數學, 0.23)  (使用, 0.08)  (觀察, 0.14)  (透過, 0.12)  (數量, 0.13)  (以及, 0.12)  (形式, 0.10)  (一種, 0.04)  (物體, 0.14)  (邏輯推理, 0.22)  (公式化, 0.21)  (結構, 0.11)  (學科, 0.13)  (公理, 0.18)  (研究, 0.08)  (科學, 0.11)  (定理, 0.15)  (符號語言, 0.27)  (抽象化, 0.19)  (建立起, 0.19)  (某種, 0.14)  (猜想, 0.19)  (計數, 0.19)  (形狀, 0.13)  (定義, 0.11)  (嚴謹, 0.18)  (這些, 0.10)  (拓展, 0.17)  (選定, 0.17)  (概念, 0.22)  (推導, 0.19)  (運動, 0.10)  (角度看, 0.22)  (屬於, 0.06)  (產生, 0.10)  (空間, 0.12) \n"
     ]
    }
   ],
   "source": [
    "## Term weighting by TF-IDF model\n",
    "## \"以及\" has same frequency count with \"數學\" but not not important in terms of weighting\n",
    "print \"Term weighting by TF-IDF: \"\n",
    "for id, score in tfidf[corpus[0]]:\n",
    "    print \"(%s, %.2f) \" % (dictionary[id], score),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What's next?\n",
    "- What can we model after finding important words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Document Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Document Summarization\n",
    "- Usage: Information discovery, relation discovery, document classification\n",
    "- Achieve by Latent Semetic Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Latent Semetic Indexing (LSI)\n",
    "- Reduce 270,000 abstracts to 200 topics\n",
    "- Unsupervised learning, same as Principle Component Analysis (PCA)\n",
    "- Achieve by Single Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Train the LSI model using the TF-IDF transformed corpus\n",
    "# Be careful! It takes up an hour to compute!\n",
    "lsi = models.LsiModel(tfidf[corpus], id2word=dictionary, num_topics=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "corpus_lsi = lsi[corpus_tfidf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=http://www.intechopen.com/source/html/40377/media/image11_w.jpg>\n",
    "<img src=http://1.bp.blogspot.com/-pgMAHiIWvuw/Tql5HIXNdRI/AAAAAAAABLI/I2zPF5cLRwQ/s1600/clust.gif>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "冰川 -0.398759466345\n",
      "冰原 -0.380382283655\n",
      "島峰 -0.380199566592\n",
      "群島 0.230939721847\n",
      "山峰 0.209200546951\n"
     ]
    }
   ],
   "source": [
    "#Latent Sementic Indexing (LSI)\n",
    "for i,j in lsi.show_topic(23)[:5]: print i,j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "棲息 0.279671065614\n",
      "可達 0.250394935748\n",
      "體長 0.236198529056\n",
      "公分 0.234641148863\n",
      "習性 0.224247181259\n"
     ]
    }
   ],
   "source": [
    "for i,j in lsi.show_topic(0)[:5]: print i,j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### LSI for text summarization: Ambiguous\n",
    "- How can we interupt those topics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Topic Modelling: Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Image source: <a href=http://ww4.sinaimg.cn/large/6cbb8645jw1eoqsai9lhyj20pk0don1k.jpg>http://ww4.sinaimg.cn/large/6cbb8645jw1eoqsai9lhyj20pk0don1k.jpg</a>\n",
    "<img src=http://ww4.sinaimg.cn/large/6cbb8645jw1eoqsai9lhyj20pk0don1k.jpg width=70% height=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Documents Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Documents Similarity\n",
    "- Given a document, how to find the most similar one from corpus?\n",
    "- Achieve by buliding similarity matrix (correlation matrix) from LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n"
     ]
    }
   ],
   "source": [
    "# Generate an index of the LSI space\n",
    "# Correlation Matrix\n",
    "index = similarities.MatrixSimilarity(corpus_lsi)\n",
    "# Query the index with a document transformed into the tf-idf space # This returns a list of tuples in the form of (score, doc_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "游戲 可以 指人 一種 娛樂活動 也可以 這種 活動 過程 游戲 道具 可以 玩具 英語 體育比賽 遊戲 一種 體育運動 遊戲 演變\n"
     ]
    }
   ],
   "source": [
    "# Input document index\n",
    "documentIndex = 40\n",
    "print \" \".join(documents[documentIndex][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sims = index[lsi[tfidf[corpus[documentIndex]]]]\n",
    "# Sort the similarity values by their scores\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# print sims\n",
    "#[(40, 1.0), (20733, 0.95213395), (94363, 0.95173347), (199968, 0.95007229), \n",
    "# (63281, 0.94930953), (55712, 0.94783223), (225236, 0.94558185), (7930, 0.94430256), \n",
    "# (63259, 0.94354367), (12311, 0.9380967), (2379, 0.93661648), (2254, 0.93655413), ......\n",
    "\n",
    "# Three most similiar topics \n",
    "print \"Index \" + str(sims[1][0]) + \": \" + \" \".join(documents[sims[1][0]][:20])\n",
    "print \"Index \" + str(sims[2][0]) + \": \" + \" \".join(documents[sims[2][0]][:20])\n",
    "print \"Index \" + str(sims[3][0]) + \": \" + \" \".join(documents[sims[3][0]][:20])                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "print index.num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Word Similarity\n",
    "- \"數學\" + \"計算機科學\" = ?\n",
    "- \"蘋果\" + \"微軟\" = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Google's word2vec\n",
    "- Proposed by Tomas Mikolov in late 2013\n",
    "- Further reading: <a href=http://suanfazu.com/t/word2vec-zhong-de-shu-xue-yuan-li-xiang-jie-duo-tu-wifixia-yue-du/178>word2vec 中的数学原理详解（多图，WIFI下阅读）</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### word2vec: Principle\n",
    "- Every word consider as a vector\n",
    "- Given input vectors (words), predict most *relavant* words\n",
    "<img src=http://1.bp.blogspot.com/-Q7F8ulD6fC0/UgvnVCSGmXI/AAAAAAAAAbg/MCWLTYBufhs/s1600/image00.gif  width=50% height=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Google's word2vec: workflow\n",
    "- Training method: \"Surfcae\" learning, two-layer neural networks<br>\n",
    "Image source: <a href=https://iksinc.files.wordpress.com/2015/04/screen-shot-2015-04-12-at-10-58-21-pm.png>https://iksinc.files.wordpress.com/2015/04/screen-shot-2015-04-12-at-10-58-21-pm.png</a>\n",
    "<img src=https://iksinc.files.wordpress.com/2015/04/screen-shot-2015-04-12-at-10-58-21-pm.png width=50% height=50%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Generate word vectors using the Word2vec algorithm\n",
    "model = Word2Vec(documents, size=200, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### model.most_similar(u\"計算機科學\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "信息科學 0.763425171375\n",
      "數理統計 0.724284648895\n",
      "理論物理 0.723909497261\n",
      "數學 0.719389021397\n",
      "統計學 0.712564647198\n",
      "材料科學 0.706905901432\n",
      "工程學 0.704143762589\n",
      "應用 0.702665686607\n",
      "計量經濟學 0.69039785862\n",
      "通信工程 0.687664985657\n"
     ]
    }
   ],
   "source": [
    "for w, score in model.most_similar(u\"計算機科學\"): print w, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### model.most_similar([u\"數學\",u\"計算機科學\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "拓撲學 0.780949652195\n",
      "理論物理 0.774173974991\n",
      "數理統計 0.76259547472\n",
      "統計學 0.750313699245\n",
      "信息科學 0.747869253159\n",
      "同倫 0.734885334969\n",
      "計量經濟學 0.732247769833\n",
      "微分方程 0.73089826107\n",
      "概率論 0.729114413261\n",
      "微積分 0.727130234241\n"
     ]
    }
   ],
   "source": [
    "for w, score in model.most_similar([u\"數學\",u\"計算機科學\"]): print w, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### model.most_similar([u\"蘋果\",u\"微軟\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "蘋果公司 0.773305773735\n",
      "智能手機 0.760377526283\n",
      "個人電腦 0.746866583824\n",
      "超薄 0.73600679636\n",
      "作業系統 0.727758347988\n",
      "系列產品 0.726376593113\n",
      "諾基亞公司 0.725656986237\n",
      "手提電腦 0.722447395325\n",
      "麥金塔 0.721516311169\n",
      "首款 0.720096707344\n"
     ]
    }
   ],
   "source": [
    "for w, score in model.most_similar(positive=[u\"蘋果\",u\"微軟\"]): print w, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### model.most_similar([u\"梁振英\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "曾蔭權 0.827244877815\n",
      "董建華 0.790594279766\n",
      "香港特區 0.78056293726\n",
      "何厚鏵 0.77780520916\n",
      "施政報告 0.777581095695\n",
      "公職 0.777228713036\n",
      "民主派 0.771535158157\n",
      "立法會 0.764261543751\n",
      "香港基本法 0.762010157108\n",
      "香港特別行政區 0.761656165123\n"
     ]
    }
   ],
   "source": [
    "for w, score in model.most_similar([u\"梁振英\"]): print w, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "電腦\n"
     ]
    }
   ],
   "source": [
    "print model.doesnt_match([u\"物理\",u\"化學\",u\"生物\",u\"電腦\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Milestone\n",
    "- Word segmentation (Trie Tree, DAG and HMM)\n",
    "- Word frequency count and weighting (TF-IDF)\n",
    "- Text summarization (LSI)\n",
    "- Search for similiar documents (LSI + Similiarity Matrix)\n",
    "- Search for similiar words (Word2Vec)\n",
    "\n",
    "## To-dos\n",
    "- Topic modelling (LDA)\n",
    "- More case studies\n",
    "- Search for potential applications "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# One more thing..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 二零一六年施政報告\n",
    "## 2016 Policy Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "二零一六年施政報告\r\n",
      "創新經濟改善民生促進和諧繁榮共享\r\n",
      "目錄\r\n",
      "(一)引言(二)經濟\r\n",
      "金融業航運及物流業旅遊業\r\n",
      "專業服務\r\n",
      "創意產業知識產權業\r\n",
      "漁農業國家「十三五」規劃加強聯繫和合作\r\n",
      "(三)「一帶一路」集資融資平台\r\n",
      "商貿物流平台\r\n",
      "貿易環境專業及基礎設施服務平台促進民心相通「一帶一路」辦公室\r\n",
      "(四)創新及科技向下游出發\r\n",
      "發展產業支援創科初創企業數碼及「智慧城市」創科生活基金其他配套\r\n",
      "段落\r\n",
      "段落\r\n",
      "(五)房屋土地與交通運輸\r\n",
      "房屋\r\n",
      "土地短中期土地供應\r\n",
      "檢討土地用途發展前礦場用地賣地計劃\r\n",
      "中長期土地供應\r\n",
      "新發展區和新市鎮擴展計劃將軍澳第區發展岩洞、地下空間及維港以外填海大嶼山\r\n",
      "商業及經濟用地\r\n",
      "九龍東\r\n",
      "啓德發展區\r\n",
      "長遠城市規劃\r\n"
     ]
    }
   ],
   "source": [
    "!head -25 PA2016.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngrams(input, n):\n",
    "  input = input.split(' ')\n",
    "  output = []\n",
    "  for i in range(len(input)-n+1):\n",
    "    output.append(input[i:i+n])\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "lines = codecs.open(\"PA2016-words.txt\", \"r\", \"utf-8\").readlines()\n",
    "words=[]\n",
    "for l in lines: \n",
    "    words += l.strip().split(\" \")\n",
    "documents.append(words)\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "corpus = [ dictionary.doc2bow(d) for d in documents ]\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "發展,服務,計劃,政府,香港,長者,加強,國家,一帶一路,資助,繼續,經濟,推動,建議,規劃,項目,企業,委員會,提供,單位,地區,醫療,相關,積極,落實,檢討,專業,內地,社會,醫院\n",
      "一帶一路\n"
     ]
    }
   ],
   "source": [
    "content = open('PA2016-words.txt', 'rb').read()\n",
    "jieba.load_userdict('PA2016-jieba.txt')\n",
    "PAtags = jieba.analyse.extract_tags(content, 30)\n",
    "print \",\".join(PAtags)\n",
    "PAtags.index(u'一帶一路')\n",
    "print \"%s\" % PAtags[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(政府, 247)  (香港, 163)  (發展, 154)  (服務, 107)  (計劃, 101)  (提供, 78)  (包括, 58)  (長者, 50)  (增加, 49)  (加強, 49)  (工作, 48)  (研究, 46)  (以及, 45)  (國家, 44)  (一帶一路, 44)  (繼續, 42)  (資助, 42)  (土地, 40)  (合作, 39)  (經濟, 38)  (需要, 36)  (建議, 36)  (推動, 36)  (項目, 34)  (規劃, 34)  (企業, 33)  (中心, 32)  (改善, 31)  (委員會, 31)  (教育, 31) \n"
     ]
    }
   ],
   "source": [
    "corpus[227364].sort(key=lambda x: x[1], reverse=True)\n",
    "for id, score in corpus[227364][:30]: print \"(%s, %d) \" % (dictionary[id], score),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(政府, 0.40)  (香港, 0.19)  (發展, 0.23)  (服務, 0.18)  (計劃, 0.19)  (提供, 0.13)  (包括, 0.07)  (長者, 0.16)  (增加, 0.11)  (加強, 0.13)  (工作, 0.08)  (研究, 0.07)  (以及, 0.05)  (國家, 0.06)  (一帶一路, 0.21)  (繼續, 0.10)  (資助, 0.11)  (土地, 0.09)  (合作, 0.08)  (經濟, 0.06)  (需要, 0.07)  (建議, 0.09)  (推動, 0.08)  (項目, 0.06)  (規劃, 0.07)  (企業, 0.06)  (中心, 0.05)  (改善, 0.08)  (委員會, 0.05)  (教育, 0.06) \n"
     ]
    }
   ],
   "source": [
    "wikiTags=[]\n",
    "for id, score in tfidf[corpus[227364]][:30]: \n",
    "    print \"(%s, %.2f) \" % (dictionary[id], score),\n",
    "    wikiTags+=dictionary[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "發展 服務 計劃 政府 香港 長者 加強 國家 一帶一路 資助 繼續 經濟 推動 建議 規劃 項目 企業 委員會 提供  地      業 地 會 \n"
     ]
    }
   ],
   "source": [
    "c3 = [filter(lambda x: x in wikiTags, sublist) for sublist in PAtags]\n",
    "for word in c3 : print \"%s\" % word,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# New word2Vec model including Policy Address\n",
    "model = Word2Vec(documents, size=200, window=5, min_count=0, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "長遠 0.702973484993\n",
      "城鄉規劃 0.688625693321\n",
      "特殊作用 0.676903486252\n",
      "擬訂 0.675324440002\n",
      "知識經濟 0.668613672256\n",
      "第十一個五年 0.668246924877\n",
      "知識型 0.664405941963\n",
      "偏鄉 0.662087261677\n",
      "願景 0.651054263115\n",
      "集體化 0.647202193737\n"
     ]
    }
   ],
   "source": [
    "for w, score in model.most_similar([u\"一帶一路\",u\"發展\"]): print w, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Reference\n",
    "Introduction to Latent Dirichlet Allocation: http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/<br>\n",
    "Dimensionality Reduction and Latent Topic Models: http://pages.cs.wisc.edu/~jerryzhu/cs769/latent.pdf <br>\n",
    "Google's word2vec: <a href=https://code.google.com/p/word2vec/>https://code.google.com/p/word2vec/</a><br>\n",
    "Latent Semantic Analysis and Topic Modeling: Roads to Text Meaning <a href=http://www.jaist.ac.jp/~bao/Writings/TopicModeling2.pdf>http://www.jaist.ac.jp/~bao/Writings/TopicModeling2.pdf</a><br>\n",
    "Latent Semantic Indexing (LSI) An Example: <a href=http://www1.se.cuhk.edu.hk/~seem5680/lecture/LSI-Eg.pdf>http://www1.se.cuhk.edu.hk/~seem5680/lecture/LSI-Eg.pdf</a><br>\n",
    "word2vec 中的数学原理详解: <a href=http://suanfazu.com/t/word2vec-zhong-de-shu-xue-yuan-li-xiang-jie-duo-tu-wifixia-yue-du/178>http://suanfazu.com/t/word2vec-zhong-de-shu-xue-yuan-li-xiang-jie-duo-tu-wifixia-yue-du/178</a><br>\n",
    "中英文维基百科语料上的Word2Vec实验: <a href=http://www.52nlp.cn/中英文维基百科语料上的word2vec实验>http://www.52nlp.cn/中英文维基百科语料上的word2vec实验</a><br>\n",
    "Matrix decompositions and latent semantic indexing: <a href=http://nlp.stanford.edu/IR-book/pdf/18lsi.pdf>http://nlp.stanford.edu/IR-book/pdf/18lsi.pdf</a><br>\n",
    "Chinese NLP with Open Source Tools in Python: <a href=https://github.com/albertauyeung/pyconhk2015-chinese-nlp>https://github.com/albertauyeung/pyconhk2015-chinese-nlp</a><br>\n",
    "JIEBA 結巴中文斷詞: <a href=https://speakerdeck.com/fukuball/jieba-jie-ba-zhong-wen-duan-ci>https://speakerdeck.com/fukuball/jieba-jie-ba-zhong-wen-duan-ci</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Applications\n",
    "- Visualization\n",
    "<img src=http://d3js.org/preview.png>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
